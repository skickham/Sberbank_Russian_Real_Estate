{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skick\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:32: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    }
   ],
   "source": [
    "# PCA on continuous numeric features\n",
    "\n",
    "# Clean the data\n",
    "#### This section will not be necessary once we have the fnalized, cleaned data\n",
    "\n",
    "# Grab the raw data\n",
    "train = pd.read_csv(\"train.csv\", parse_dates=['timestamp']).drop(['id'],axis=1)\n",
    "test_raw= pd.read_csv(\"test.csv\", parse_dates=['timestamp']).drop(['id'],axis=1)\n",
    "macro= pd.read_csv(\"macro.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "\n",
    "# Merge the data (if we choose to merge a lag on the macro, this could be even better)\n",
    "train['dataset'] = 'train'\n",
    "test_raw['dataset'] = 'test'\n",
    "df = pd.concat([train, test_raw])\n",
    "df = pd.merge(df, macro, on = 'timestamp', how='left')\n",
    "\n",
    "\n",
    "# Log transform skewed numeric features \n",
    "get_col = df.dtypes[(df.dtypes == \"int64\") | (df.dtypes == \"float64\")].index\n",
    "get_skews = df[get_col].apply(lambda x: skew(x.dropna()))\n",
    "get_skews = get_skews[get_skews>0.5]\n",
    "get_skews = get_skews.index\n",
    "df[get_skews] = np.log1p(df[get_skews])    \n",
    "\n",
    "\n",
    "# select continuous numeric columns\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_df = df.select_dtypes(include=numerics)\n",
    "num_df.shape # 364 features\n",
    "\n",
    "\n",
    "# impute missing values with mean\n",
    "df = num_df.apply(lambda x: x.fillna(x.mean()),axis=0) # newdf is the numeric columns\n",
    "\n",
    "\n",
    "# scale the data frame\n",
    "df = (df - df.mean()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC for market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.71537854] & sum =  0.715378536052\n"
     ]
    }
   ],
   "source": [
    "# one ==> 96%, two ==> 98%\n",
    "\n",
    "market = ['market_count_500','market_count_5000', 'market_count_2000',\n",
    "          'market_count_1000','market_count_1500','market_count_3000']    \n",
    "df_pc = df[market]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=1)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['market_pc_1'])  # for first one, create a new dataframe\n",
    "df = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC for cafes (not useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # maybe not a good one  (4 to get over 90%)\n",
    "\n",
    "# cafe = ['cafe_sum_500_min_price_avg', 'cafe_sum_500_max_price_avg',\n",
    "#         'cafe_avg_price_500', 'cafe_sum_1000_min_price_avg','cafe_sum_1000_max_price_avg', \n",
    "#         'cafe_avg_price_1000', 'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg', \n",
    "#         'cafe_avg_price_1500', 'cafe_sum_2000_min_price_avg', 'cafe_sum_2000_max_price_avg', \n",
    "#         'cafe_avg_price_2000', 'cafe_sum_3000_min_price_avg', 'cafe_sum_3000_max_price_avg',\n",
    "#         'cafe_avg_price_3000',  'cafe_sum_5000_min_price_avg', 'cafe_sum_5000_max_price_avg',\n",
    "#         'cafe_avg_price_5000','cafe_count_5000_price_high','cafe_count_500', \n",
    "#         'cafe_count_500_na_price','cafe_count_500_price_500', 'cafe_count_500_price_1000',\n",
    "#         'cafe_count_500_price_1500', 'cafe_count_500_price_2500',\n",
    "#         'cafe_count_500_price_4000', 'cafe_count_500_price_high', 'cafe_count_1000', \n",
    "#         'cafe_count_1000_na_price', 'cafe_count_1000_price_500',\n",
    "#         'cafe_count_1000_price_1000', 'cafe_count_1000_price_1500',\n",
    "#         'cafe_count_1000_price_2500', 'cafe_count_1000_price_4000',\n",
    "#         'cafe_count_1000_price_high','cafe_count_1500', 'cafe_count_1500_na_price', \n",
    "#         'cafe_count_1500_price_500', 'cafe_count_1500_price_1000',\n",
    "#         'cafe_count_1500_price_1500', 'cafe_count_1500_price_2500',\n",
    "#         'cafe_count_1500_price_4000', 'cafe_count_1500_price_high', 'cafe_count_2000', \n",
    "#         'cafe_count_2000_na_price', 'cafe_count_2000_price_500',\n",
    "#         'cafe_count_2000_price_1000', 'cafe_count_2000_price_1500',\n",
    "#         'cafe_count_2000_price_2500', 'cafe_count_2000_price_4000',\n",
    "#         'cafe_count_2000_price_high', 'cafe_count_3000', 'cafe_count_3000_na_price',\n",
    "#         'cafe_count_3000_price_500', 'cafe_count_3000_price_1000',\n",
    "#         'cafe_count_3000_price_1500', 'cafe_count_3000_price_2500',\n",
    "#         'cafe_count_3000_price_4000', 'cafe_count_3000_price_high','cafe_count_5000',\n",
    "#         'cafe_count_5000_na_price', 'cafe_count_5000_price_500',\n",
    "#         'cafe_count_5000_price_1000', 'cafe_count_5000_price_1500',\n",
    "#         'cafe_count_5000_price_2500', 'cafe_count_5000_price_4000', 'cafe_count_5000_price_high']\n",
    "\n",
    "# df_pc = df[cafe]\n",
    "\n",
    "\n",
    "# #set params & fit model\n",
    "# pca.set_params(n_components=4)\n",
    "# pca.fit(df_pc)\n",
    "\n",
    "# # find variance contained\n",
    "# print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# # give eigenvalues\n",
    "  \n",
    "\n",
    "# # apply projection\n",
    "# num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# # Join back with original data\n",
    "# num_df2 = pd.DataFrame(num_df2)\n",
    "# df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.59585927  0.29209367  0.07902603] & sum =  0.96697897201\n"
     ]
    }
   ],
   "source": [
    "# 2 for 90%, 3 for 97%\n",
    "\n",
    "green = ['green_part_500', 'green_part_1000','green_part_1500',\n",
    "         'green_part_2000','green_part_3000','green_part_5000']\n",
    "\n",
    "\n",
    "df_pc = df[green]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['green_pc_1', 'green_pc_2', 'green_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Industrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.71203615  0.168758    0.07204646] & sum =  0.952840608634\n"
     ]
    }
   ],
   "source": [
    "# 2 for 89.7%, 3 for 96%\n",
    "\n",
    "prom = ['prom_part_500','prom_part_1000','prom_part_1500',\n",
    "        'prom_part_2000','prom_part_3000','prom_part_5000']\n",
    "\n",
    "df_pc = df[prom]\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['prom_pc_1', 'prom_pc_2', 'prom_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.73173197  0.10750383  0.06000975] & sum =  0.899245554034\n"
     ]
    }
   ],
   "source": [
    "# 3 for 90%, 4 for 95% \n",
    "\n",
    "office = ['office_count_500','office_sqm_500','office_count_1000',\n",
    "          'office_sqm_1000','office_count_1500', 'office_sqm_1500',\n",
    "          'office_count_2000','office_sqm_2000','office_count_3000',\n",
    "          'office_sqm_3000','office_count_5000','office_sqm_5000']\n",
    "  \n",
    "\n",
    "df_pc = df[office]\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['office_pc_1', 'office_pc_2', 'office_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC cultural (not useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # maybe not a good one, needs 8 for over 90%\n",
    "\n",
    "\n",
    "# # grouped cultural characteristics\n",
    "# cult_chars = ['sport_objects_raion', 'culture_objects_top_25_raion', 'shopping_centers_raion', 'park_km', 'fitness_km', \n",
    "#                 'swim_pool_km', 'ice_rink_km','stadium_km', 'basketball_km', 'shopping_centers_km', 'big_church_km',\n",
    "#                 'church_synagogue_km', 'mosque_km', 'theater_km', 'museum_km', 'exhibition_km', 'catering_km']\n",
    "\n",
    "# df_pc = df[cult_chars]\n",
    "\n",
    "\n",
    "# pca.set_params(n_components=7)\n",
    "# pca.fit(df_pc)\n",
    "\n",
    "# print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))  \n",
    "# # give eigenvalues\n",
    "\n",
    "# # df['cafe_pca'] = pca.transform(df_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC shopping malls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.69485217  0.12380419  0.07313241] & sum =  0.891788768601\n"
     ]
    }
   ],
   "source": [
    "# 3 for 91%, 4 for 95%\n",
    "\n",
    "trc = ['trc_count_1000', 'trc_count_1500', 'trc_count_2000', 'trc_count_3000', \n",
    "       'trc_count_500', 'trc_count_5000', 'trc_sqm_1000', 'trc_sqm_1500',\n",
    "       'trc_sqm_2000', 'trc_sqm_3000', 'trc_sqm_500', 'trc_sqm_5000', 'trc_count_1000', 'trc_sqm_1000']\n",
    "\n",
    "df_pc = df[trc]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns=['trc_pc_1', 'trc_pc_2', 'trc_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC religious buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.55307382  0.15573631  0.08765663] & sum =  0.796466758515\n"
     ]
    }
   ],
   "source": [
    "# 3 for 89.6%, 7 for 95%\n",
    "\n",
    "church = ['big_church_count_500', 'church_count_500', 'mosque_count_500',\n",
    "         'big_church_count_1000', 'church_count_1000', 'mosque_count_1000',\n",
    "         'big_church_count_1500', 'church_count_1500', 'mosque_count_1500',\n",
    "         'big_church_count_3000', 'church_count_3000', 'mosque_count_3000',\n",
    "         'big_church_count_5000', 'church_count_5000', 'mosque_count_5000',\n",
    "         'big_church_count_2000', 'church_count_2000', 'mosque_count_2000']\n",
    "\n",
    "df_pc = df[church]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['church_pc_1', 'church_pc_2', 'church_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.82110102  0.10705881] & sum =  0.928159827462\n"
     ]
    }
   ],
   "source": [
    "# 2 for 93.8%, 3 for 96.7%\n",
    "\n",
    "sport = ['sport_count_500','sport_count_1000','sport_count_2000', \n",
    "         'sport_count_5000','sport_count_1500','sport_count_3000']\n",
    "\n",
    "df_pc = df[sport]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=2)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['sport_pc_1', 'sport_pc_2'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC leisure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.83712373  0.09507349] & sum =  0.932197219427\n"
     ]
    }
   ],
   "source": [
    "# 2 for 95%\n",
    "\n",
    "leisure = ['leisure_count_500','leisure_count_3000','leisure_count_1000',\n",
    "           'leisure_count_1500','leisure_count_2000','leisure_count_5000']\n",
    "\n",
    "\n",
    "df_pc = df[leisure]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=2)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['leisure_pc_1', 'leisure_pc_2'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.55939771  0.13198228  0.0900099 ] & sum =  0.78138989746\n"
     ]
    }
   ],
   "source": [
    "# 2 for 92%, 3 for 95.1%\n",
    "\n",
    "school = ['children_preschool', 'preschool_quota', 'preschool_education_centers_raion', 'children_school', \n",
    "                'school_quota', 'school_education_centers_raion', 'school_education_centers_top_20_raion', \n",
    "                'university_top_20_raion', 'additional_education_raion', 'additional_education_km', 'university_km']\n",
    "\n",
    "\n",
    "df_pc = df[school]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['school_pc_1', 'school_pc_2', 'school_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC cafe price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.44775356  0.17121346  0.1518611   0.1001934   0.05726982] & sum =  0.928291332225\n"
     ]
    }
   ],
   "source": [
    "# 5 for 92%\n",
    "\n",
    "cafe_price = ['cafe_sum_500_min_price_avg', 'cafe_sum_500_max_price_avg',\n",
    "              'cafe_avg_price_500', 'cafe_sum_1000_min_price_avg','cafe_sum_1000_max_price_avg', \n",
    "              'cafe_avg_price_1000', 'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg', \n",
    "              'cafe_avg_price_1500', 'cafe_sum_2000_min_price_avg', 'cafe_sum_2000_max_price_avg', \n",
    "              'cafe_avg_price_2000', 'cafe_sum_3000_min_price_avg', 'cafe_sum_3000_max_price_avg',\n",
    "              'cafe_avg_price_3000',  'cafe_sum_5000_min_price_avg', 'cafe_sum_5000_max_price_avg',\n",
    "              'cafe_avg_price_5000', 'cafe_count_5000_price_high']      \n",
    "\n",
    "df_pc = df[cafe_price]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=5)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, \n",
    "                       columns = ['cafe_price_pc_1', 'cafe_price_pc2', \n",
    "                                  'cafe_price_pc_3', 'cafe_price_pc_4', \n",
    "                                  'cafe_price_pc_5'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC cafe count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [ 0.77123331  0.07251101  0.03472481] & sum =  0.878469128477\n"
     ]
    }
   ],
   "source": [
    "# 3 for 90.9%, 8 to get over 95%\n",
    "\n",
    "cafe_count = ['cafe_count_500', 'cafe_count_500_na_price', 'cafe_count_500_price_500', \n",
    "              'cafe_count_500_price_1000', 'cafe_count_500_price_1500', 'cafe_count_500_price_2500',\n",
    "              'cafe_count_500_price_4000', 'cafe_count_500_price_high', 'cafe_count_1000', \n",
    "              'cafe_count_1000_na_price', 'cafe_count_1000_price_500', 'cafe_count_1000_price_1000',\n",
    "              'cafe_count_1000_price_1500', 'cafe_count_1000_price_2500', 'cafe_count_1000_price_4000',\n",
    "              'cafe_count_1000_price_high','cafe_count_1500', 'cafe_count_1500_na_price', 'cafe_count_1500_price_500',\n",
    "              'cafe_count_1500_price_1000', 'cafe_count_1500_price_1500', 'cafe_count_1500_price_2500',\n",
    "              'cafe_count_1500_price_4000', 'cafe_count_1500_price_high', 'cafe_count_2000', \n",
    "              'cafe_count_2000_na_price', 'cafe_count_2000_price_500', 'cafe_count_2000_price_1000',\n",
    "              'cafe_count_2000_price_1500', 'cafe_count_2000_price_2500', 'cafe_count_2000_price_4000',\n",
    "              'cafe_count_2000_price_high', 'cafe_count_3000', 'cafe_count_3000_na_price',\n",
    "              'cafe_count_3000_price_500', 'cafe_count_3000_price_1000', 'cafe_count_3000_price_1500', \n",
    "              'cafe_count_3000_price_2500', 'cafe_count_3000_price_4000', 'cafe_count_3000_price_high',\n",
    "              'cafe_count_5000', 'cafe_count_5000_na_price', 'cafe_count_5000_price_500',\n",
    "              'cafe_count_5000_price_1000', 'cafe_count_5000_price_1500', 'cafe_count_5000_price_2500', \n",
    "              'cafe_count_5000_price_4000', 'cafe_count_5000_price_high'] \n",
    "\n",
    "df_pc = df[cafe_count]\n",
    "\n",
    "\n",
    "#set params & fit model\n",
    "pca.set_params(n_components=3)\n",
    "pca.fit(df_pc)\n",
    "\n",
    "\n",
    "# find variance contained\n",
    "print('Variance Ratio: ' + str(pca.explained_variance_ratio_) + ' & sum =  ' + str(sum(pca.explained_variance_ratio_)))\n",
    "# give eigenvalues\n",
    "  \n",
    "\n",
    "# apply projection\n",
    "num_df2 = pca.transform(df_pc)\n",
    "\n",
    "\n",
    "# Join back with original data\n",
    "num_df2 = pd.DataFrame(num_df2, columns = ['cafe_count_pc_1', 'cafe_count_pc_2', 'cafe_count_pc_3'])\n",
    "df  = pd.concat([df, num_df2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# im sure there was a better way to do this \n",
    "\n",
    "\n",
    "drop = ['green_part_500', 'green_part_1000','green_part_1500',\n",
    "        'green_part_2000','green_part_3000','green_part_5000', \n",
    "        'prom_part_500','prom_part_1000','prom_part_1500',\n",
    "        'prom_part_2000','prom_part_3000','prom_part_5000', \n",
    "        'office_count_500','office_sqm_500','office_count_1000',\n",
    "        'office_sqm_1000','office_count_1500', 'office_sqm_1500',\n",
    "        'office_count_2000','office_sqm_2000','office_count_3000',\n",
    "        'office_sqm_3000','office_count_5000','office_sqm_5000', \n",
    "        'sport_objects_raion', 'culture_objects_top_25_raion', 'shopping_centers_raion', 'park_km', 'fitness_km', \n",
    "        'swim_pool_km', 'ice_rink_km','stadium_km', 'basketball_km', 'shopping_centers_km', 'big_church_km',\n",
    "        'church_synagogue_km', 'mosque_km', 'theater_km', 'museum_km', 'exhibition_km', 'catering_km',\n",
    "        'trc_count_1000', 'trc_count_1500', 'trc_count_2000', 'trc_count_3000', \n",
    "        'trc_count_500', 'trc_count_5000', 'trc_sqm_1000', 'trc_sqm_1500',\n",
    "        'trc_sqm_2000', 'trc_sqm_3000', 'trc_sqm_500', 'trc_sqm_5000', 'trc_count_1000', 'trc_sqm_1000',\n",
    "        'big_church_count_500', 'church_count_500', 'mosque_count_500',\n",
    "        'big_church_count_1000', 'church_count_1000', 'mosque_count_1000',\n",
    "        'big_church_count_1500', 'church_count_1500', 'mosque_count_1500',\n",
    "        'big_church_count_3000', 'church_count_3000', 'mosque_count_3000',\n",
    "        'big_church_count_5000', 'church_count_5000', 'mosque_count_5000',\n",
    "        'big_church_count_2000', 'church_count_2000', 'mosque_count_2000',\n",
    "        'sport_count_500','sport_count_1000','sport_count_2000', \n",
    "        'sport_count_5000','sport_count_1500','sport_count_3000',\n",
    "        'leisure_count_500','leisure_count_3000','leisure_count_1000',\n",
    "        'leisure_count_1500','leisure_count_2000','leisure_count_5000',\n",
    "        'market_count_500','market_count_5000', 'market_count_2000',\n",
    "        'market_count_1000','market_count_1500','market_count_3000',\n",
    "        'children_preschool', 'preschool_quota', 'preschool_education_centers_raion', 'children_school', \n",
    "        'school_quota', 'school_education_centers_raion', 'school_education_centers_top_20_raion', \n",
    "        'university_top_20_raion', 'additional_education_raion', 'additional_education_km', 'university_km',\n",
    "        'cafe_sum_500_min_price_avg', 'cafe_sum_500_max_price_avg',\n",
    "        'cafe_avg_price_500', 'cafe_sum_1000_min_price_avg','cafe_sum_1000_max_price_avg', \n",
    "        'cafe_avg_price_1000', 'cafe_sum_1500_min_price_avg', 'cafe_sum_1500_max_price_avg', \n",
    "        'cafe_avg_price_1500', 'cafe_sum_2000_min_price_avg', 'cafe_sum_2000_max_price_avg', \n",
    "        'cafe_avg_price_2000', 'cafe_sum_3000_min_price_avg', 'cafe_sum_3000_max_price_avg',\n",
    "        'cafe_avg_price_3000',  'cafe_sum_5000_min_price_avg', 'cafe_sum_5000_max_price_avg',\n",
    "        'cafe_avg_price_5000','cafe_count_5000_price_high',\n",
    "        'cafe_count_500', 'cafe_count_500_na_price',\n",
    "        'cafe_count_500_price_500', 'cafe_count_500_price_1000',\n",
    "        'cafe_count_500_price_1500', 'cafe_count_500_price_2500',\n",
    "        'cafe_count_500_price_4000', 'cafe_count_500_price_high', 'cafe_count_1000', \n",
    "        'cafe_count_1000_na_price', 'cafe_count_1000_price_500',\n",
    "        'cafe_count_1000_price_1000', 'cafe_count_1000_price_1500',\n",
    "        'cafe_count_1000_price_2500', 'cafe_count_1000_price_4000',\n",
    "        'cafe_count_1000_price_high','cafe_count_1500','cafe_count_1500_na_price',\n",
    "        'cafe_count_1500_price_500', 'cafe_count_1500_price_1000',\n",
    "        'cafe_count_1500_price_1500', 'cafe_count_1500_price_2500',\n",
    "        'cafe_count_1500_price_4000', 'cafe_count_1500_price_high', 'cafe_count_2000', \n",
    "        'cafe_count_2000_na_price', 'cafe_count_2000_price_500',\n",
    "        'cafe_count_2000_price_1000', 'cafe_count_2000_price_1500',\n",
    "        'cafe_count_2000_price_2500', 'cafe_count_2000_price_4000',\n",
    "        'cafe_count_2000_price_high', 'cafe_count_3000', 'cafe_count_3000_na_price',\n",
    "        'cafe_count_3000_price_500', 'cafe_count_3000_price_1000',\n",
    "        'cafe_count_3000_price_1500', 'cafe_count_3000_price_2500',\n",
    "        'cafe_count_3000_price_4000', 'cafe_count_3000_price_high','cafe_count_5000',\n",
    "        'cafe_count_5000_na_price', 'cafe_count_5000_price_500',\n",
    "        'cafe_count_5000_price_1000', 'cafe_count_5000_price_1500',\n",
    "        'cafe_count_5000_price_2500', 'cafe_count_5000_price_4000',\n",
    "        'cafe_count_5000_price_high'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop(df[drop], axis = 1)\n",
    "\n",
    "df.to_csv('pca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC all components (not useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pca = PCA()\n",
    "# pca.set_params(n_components = 30) \n",
    "# pca.fit(num_df)\n",
    "\n",
    "# print('Variance Ratio: ' + str(pca.explained_variance_ratio_) +\n",
    "#       ' & sum = ' + str(sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# num_df2 = pca.transform(num_df)\n",
    "# print(num_df.shape)\n",
    "# print(num_df2.shape)\n",
    "# num_df\n",
    "# pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print(df.shape)\n",
    "\n",
    "# df_3 = df\n",
    "# num_df2 = pd.DataFrame(num_df2) ;\n",
    "\n",
    "# df_3  = pd.concat([df_3, num_df2], axis = 1)\n",
    "# # it worked!\n",
    "\n",
    "# df_3 = df_3.drop(df.select_dtypes(include=numerics), axis = 1)\n",
    "# df_3.shape # all observations but only 44 variables of numeric;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Perform PCA\n",
    "# pca = PCA()\n",
    "# pca.set_params(n_components = 30)   # 30 was arbitrary due to high dimensions...feel free to change\n",
    "# pca.fit(num_df)\n",
    "\n",
    "\n",
    "# # Transform/ project observations onto loading vectors\n",
    "# num_df2 = pca.transform(num_df)\n",
    "\n",
    "\n",
    "# # Join back with original data\n",
    "# num_df2 = pd.DataFrame(num_df2)\n",
    "# df_pc  = pd.concat([df, num_df2], axis = 1)\n",
    "\n",
    "\n",
    "# # Drop continuous numeric features in favor of the PCs\n",
    "# df = df_pc.drop(df.select_dtypes(include=numerics), axis = 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
