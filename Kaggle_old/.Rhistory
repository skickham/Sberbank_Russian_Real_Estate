conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
lines(newdata$Bwt, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
predict(model, newdata, interval = "confidence")  # use the model as anchor for new data and construct conf interval
newdata = data.frame(Bwt = c(2.8, 5, 10))   # init new body weights
predict(model, newdata, interval = "confidence")  # use the model as anchor for new data and construct conf interval
predict(model, newdata, interval = "prediction")
boxcox(model)
bc$x[which(bc$y == max(bc$y))]
box = boxcox(model)
box$x[which(box$y == max(box$y))]
Hwt.box = (cats$Hwt^lambda - 1)/lambda #Applying the Box-Cox transformation.
lambda = box$x[which(box$y == max(box$y))]
Hwt.box = (cats$Hwt^lambda - 1)/lambda #Applying the Box-Cox transformation.
model.box = lm(Hwt.box ~ cats$Bwt)
plot(model.box)
plot(cats$Bwt, Hwt.box, xlab = "Body Weight", ylab = "BC Heart Weight",
main = "Box-Cox Transformed Data")
abline(model.box, lty = 2)
summary(model.box)
plot(model.box)
boxCox(model.box)
boxcox(model.box)
rm(list = ls())
library(car)
library(dplyr)
install.packages("Sleuth2")
library(Sleuth2)
case2002
head(case2002)
?case2002
library(car)
library(dplyr)
library(Sleuth2)
head(case2002)
?case2002
summary(case2002)
plot(case2002)
sapply(case2002, sd)
sapply(case2002, class)
nrow(case2002)
plot(case2002, col = case2002$LC + 2)
plot(case2002)
plot(case2002, col = case2002$LC)
plot(case2002[c('AG', 'YR', 'CD')], col = case2002$LC)
plot(case2002[c('AG', 'YR', 'CD')], col = case2002$LC + 2)
plot(case2002[c('AG', 'YR', 'CD')], col = case2002$LC + 3)
plot(case2002[c('AG', 'YR', 'CD')], col = case2002$LC + 1)
plot(case2002[c('AG', 'YR', 'CD')], col = case2002$LC)
bad.model = lm(LC ~ ., data = GradSchools)
summary(bad.model) #Looks like everything is significant, so what's bad?
bad.model = lm(LC ~ ., data = case2002)
summary(bad.model) #Looks like everything is significant, so what's bad?
bad.model = lm(LC ~ . - SS - BK, data = case2002)
plot(bad.model) #Severe violations to the assumptions of linear regression.
logit.overall = glm(LC ~ .,
family = "binomial",   #this fam is for logistic
data = case2002)
plot(logit.overall)
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Lung Cancer")   # residual plot is not that useful in general tho, so dont spend too much time on it
plot(logit.overall)
scatter.smooth(logit.overall$fit,
residuals(logit.overall, type = "deviance"),
lpars = list(col = "red"),
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Lung Cancer")   # residual plot is not that useful in general tho, so dont spend too much time on it
abline(h = 0, lty = 2)
influencePlot(logit.overall)
summary(logit.overall)
scatter.smooth(logit.overall$fit,
residuals(logit.overall, type = "deviance"),
lpars = list(col = "red"),
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Lung Cancer")   # residual plot is not that useful in general tho, so dont spend too much time on it
abline(h = 0, lty = 2)
influencePlot(logit.overall)
confint(logit.overall)
confint.default(logit.overall)
summary(logit.overall)
logit.noBK = glm(LC ~ . - BK,
family = "binomial",
data = case2002)
plot(logit.noBK)
summary(logit.noBK)
reduced.deviance = logit.noBK$deviance #Comparing the deviance of the reduced
reduced.df = logit.noBK$df.residual    #model (the one without rank) to...
full.deviance = logit.overall$deviance #...the deviance of the full model (the
full.df = logit.overall$df.residual    #one with the rank terms).
pchisq(reduced.deviance - full.deviance,   # diff of dev
reduced.df - full.df,              # diff of resids
lower.tail = FALSE)   # p-value small, so full model is better (dont get why)
anova(logit.noBK, logit.overall, test = "Chisq")
logit.reduced = glm(LC ~ YR + BK,
family = "binomial",
data = case2002)
summary(logit.reduced)
anova(logit.reduced, logit.overall, test = "chisq")
anova(logit.reduced, logit.overall, test = "Chisq")
AIC(logit.overall, logit.noBK, logit.reduced)
BIC(logit.overall, logit.noBK, logit.reduced)
1 - logit.overall$deviance/logit.overall$null.deviance
1 - logit.noBK$deviance/logit.noBK$null.deviance
1 - logit.reduced$deviance/logit.reduced$null.deviance
table(LC.predicted)
LC.predicted = round(logit.overall$fitted.values)    # with c = 50%
table(LC.predicted)
table(truth = case2002$LC, prediction = LC.predicted)
LC.predicted = round(logit.overall$fitted.values)    # with c = 50%
table(truth = case2002$LC, prediction = LC.predicted)
107/147
newdata = with(case2002, data.frame(YR = mean(YR),
BK = factor(0:1)))
predict(logit.reduced, newdata) #This gives us the log odds; but we want
factor(case2002$BK)
levels(case2002$BK)
newdata = with(case2002, data.frame(YR = mean(YR),
BK = levels(case2002$BK))
predict(logit.reduced, newdata) #This gives us the log odds; but we want
#the probabilities.
#Using the formula to convert to probabilities:
exp(predict(logit.overall, newdata))/(1 + exp(predict(logit.overall, newdata)))
#Setting the type to "response" converts the predictions to probabilities for
#us automatically:
predict(logit.overall, newdata, type = "response")
#this predicts the rejection rates....why???
# admittance started out as interger and the formula conveted it to a factors with base level 0,
# so we need to relvel the base level to be one in the factors
relevel(as.factor(GradSchools$admit), ref = '1')
#Making it easier to see the effects of the rank variable by printing out the
#results side-by-side:
cbind(newdata, "Prob. Admitted" = predict(logit.overall, newdata, type = "response"))
# so students from top-ranked schools have higher chance of getting in
#1: prob LC with avg yr smoking and with or without BK
#2: prob LC with 0 yr smoking and with or without BK
# xiii : use latest model to classify training set and comment on how well model performs compared to baseline
LC.predicted = round(logit.overall$fitted.values)    # with c = 50%
table(truth = case2002$LC, prediction = LC.predicted)
newdata = with(case2002, data.frame(YR = mean(YR),
BK = levels(case2002$BK)))
predict(logit.reduced, newdata) #This gives us the log odds; but we want
levels(case2002$BK)
newdata = with(case2002, data.frame(YR = mean(YR),
BK = c("noBird", "Bird")))
predict(logit.reduced, newdata, type = "response") #This gives us the log odds; but we want
newdata = with(case2002, data.frame(YR = mean(YR),
BK = c("NoBird", "Bird")))
predict(logit.reduced, newdata, type = "response") # response to not give logs
newdata = with(case2002, data.frame(YR = 0,
BK = c("NoBird", "Bird")))
predict(logit.reduced, newdata, type = "response")
rm(list=ls())
library(ISLR)
library(tree)
head(OJ)
summary(OJ)
attach(OJ)
set.seed(0)
set.seed(0)
train = sample(1:nrow(OJ), 0.8*nrow(OJ))
OJ.test = OJ[-train,]
Purchase.test = Purchase[-train]
tree.OJ = tree(Purchase ~., split = 'Gini', data = OJ)
tree.OJ = tree(Purchase ~., split = 'gini', data = OJ)
plot(tree.OJ)
text(tree.OJ, pretty = 0)
summary(tree.OJ)
tree.predict = predict(tree.OJ, OJ.test, type = 'class')
purchase.test = Purchase[-train]
table(tree.predict, purchase.test)
(120+61)/(120+11+22+61)
set.seed(0)
set.seed(0)
cv.OJ = cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
par(mfrow = c(1,2))
plot(cv.OJ$size, cv.OJ$size, type = 'b',
xlab = 'Terminal Nodes', ylab = 'Misclassified Information')
plot(cv.OJ$size, cv.OJ$dev, type = 'b',
xlab = 'Terminal Nodes', ylab = 'Misclassified Information')
par(mfrow = c(1,2))
plot(cv.OJ$size, cv.OJ$dev, type = 'b',
xlab = 'Terminal Nodes', ylab = 'Misclassified Observations')
plot(cv.OJ$k, cv.OJ$dev, type = 'b',
xlab= 'Alpha', ylab = 'Misclassified Observations')
par(mfrow = c(1,2))
plot(cv.OJ$size, cv.OJ$dev, type = 'b',
xlab = 'Terminal Nodes', ylab = 'Misclassified Observations')
plot(cv.OJ$k, cv.OJ$dev, type = 'b',
xlab= 'Alpha', ylab = 'Misclassified Observations')
prune.OJ = prune.misclass(tree.OJ, best = 4)
par(mfrow = c(1,1))
summary(prune.OJ)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
tree.pred2 = predict(prune.OJ, OJ.test, type = 'class')
table(tree.pred2, purchase.test)
(109 + 53)/(109+22+30+53)
library(randomForest)
set.seed(0)
rf.OJ = randomForest(Purchase ~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
set.seed(0)
rf.OJ = randomForest(Purchase ~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
1 - 0.14
rf.pred = predict(rf.OJ, OJ.test, type='class')
table(rf.pred, Purchase.test)
(113+60)/(113+60+18+23)
importance(rf.OJ)
varImpPlot(rf.OJ)
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$err.rate[500,1]  #training 500 trees, default is 500
cat("We're performing iteration", mtry, "\n")
}
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
min(oob.err)
1 - min(oob.err)
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = 2)
pred = predict(fit, OJ.test)
table(pred, Purchase.test)
(114 + 58)/(114 + 17 + 25 + 58)
set.seed(0)
set.seed(0)
fit2 = randomForest(Purchase ~ ., data = OJ[-train, ], mtry = 2)
pred2 = predict(fit2, OJ.test)
table(pred2, Purchase.test)
(125 + 75)/(125 + 6 + 8 + 75)
library(gbm)
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase, mode =
"numeric") - 1
OJ.train.indicator = train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(train$Purchase, mode =
"numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase, mode =
"numeric") - 1
set.seed(0)
boost.OJ = gbm(Purchase ~ ., data = OJ.train,
distribution = "bernoulli",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001)
library(gbm)
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = n.trees,
type="response")
par(mfrow = c(1, 1))
berr = with(OJ.test.indicator, apply((predmat - Purchase)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(berr), col = "red")
abline(h = min(oob.err), col= "blue")
abline(h = 1-0.785, col="green")
rm(list= ls())
install.packages('arules')
install.packages("xgboost")
library(ISLR)
library(tree)
library(randomForest)
library(gbm) # wont load
head(OJ)
summary(OJ)
attach(OJ)
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10)
OJ.train = OJ[train, ]
OJ.test = OJ[-train,]
purchase.test = Purchase[-train]
set.seed(0)
tree.OJ = tree(Purchase ~., split = 'gini', data = OJ.train)    #make sure its training data!!!
plot(tree.OJ)
text(tree.OJ, pretty = 0)
summary(tree.OJ)
set.seed(0)
tree.predict = predict(tree.OJ, OJ.test, type = 'class')
table(tree.predict, OJ.test$Purchase)
(103 + 60)/(103 + 60 + 28 + 23)
set.seed(0)
cv.OJ = cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
par(mfrow = c(1,2))
plot(cv.OJ$size, cv.OJ$dev, type = 'b',
xlab = 'Terminal Nodes', ylab = 'Misclassified Observations')
plot(cv.OJ$k, cv.OJ$dev, type = 'b',
xlab= 'Alpha', ylab = 'Misclassified Observations')
best.nodes = cv.OJ$size[which(cv.OJ$dev == min(cv.OJ$dev))]
prune.OJ = prune.misclass(tree.OJ, best = best.nodes)
par(mfrow = c(1,1))
summary(prune.OJ)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
tree.pred2 = predict(prune.OJ, OJ.test, type = 'class')
table(tree.pred2, OJ.test$Purchase)
(113 + 57)/(113+57+26+18)
set.seed(0)
rf.OJ = randomForest(Purchase ~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
rf.pred = predict(rf.OJ, OJ.test, type='class')
table(rf.pred, OJ.test$Purchase)
(113+60)/nrow(OJ.test)
importance(rf.OJ) # LoyalCH   (loyalty to Citrus Hill)
varImpPlot(rf.OJ) # LoyalCH
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$err.rate[500,1]  #training 500 trees, default is 500
cat("We're performing iteration", mtry, "\n")
}
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
1 - min(oob.err)  # 81%
which(oob.err == min(oob.err)) #with 2 variables (from graph too)
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = 2)
pred = predict(fit, OJ.test)
table(pred, OJ.test$Purchase)
(114 + 58)/(114 + 17 + 25 + 58) # 80%, 2 variables
1 - oob.err[17]
set.seed(0)
fit2 = randomForest(Purchase ~ ., data = OJ.train, mtry = 2)
pred2 = predict(fit2, OJ.test, type = 'class')
table(pred2, OJ.test$Purchase)
(117 + 58)/(117+58 +14+25) # 82% with 2 variables
set.seed(0)
rf.bagged = randomForest(Purchase ~ ., data = OJ.train, mtry = 17)
table(predict(rf.bagged, OJ.test, type = 'class'), OJ.test$Purchase)
(110+62)/(172+42)   #80.37%
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase,
mode = "numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase,
mode = "numeric") - 1
set.seed(0)
boost.OJ = gbm(Purchase ~ ., data = OJ.train.indicator,
distribution = "bernoulli",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001)
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.OJ,
newdata = OJ.test.indicator,
n.trees = n.trees,
type="response")
predmat = round(predmat)
accuracy.boost = numeric(100)
for (i in 1:100) {
accuracy.boost[i] =
sum(diag(table(OJ.test.indicator$Purchase, predmat[, i]))) / 214
}
min(which(accuracy.boost == max(accuracy.boost)) * 100)
acc=rep(0,100)
for (i in 1:100){
acc[i]<-length(which(predmat[,i]==OJ.test.indicator$Purchase))/length(OJ.test.indicator$Purchase)
}
max(acc)
which.max(acc)
plot(n.trees,
accuracy.boost,
pch = 16,
type = 'b',
ylab = "Accuracy",
xlab = "# Trees",
main = "Boosted Accuracy")
abline(h = max(accuracy.boost), lty = 2) #Boosting
abline(h = (1 - min(oob.err)), col = "red3", lty = 2) #Random forests.
abline(h = (113 + 57)/nrow(OJ.test), col = "blue", lty = 2) #Pruned tree.
legend("bottomright",
c("Boosting", "Random Forests", "Pruned Tree"),
lwd = 2,
lty = 2,
col = c("black", "red3", "blue"))
wine = read.csv("[10] Wine Quality.csv")
quality = ifelse(wine$quality <= 5, "Low", "High")
wine.scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
set.seed(0)
train = sample(1:nrow(wine), 8*nrow(wine)/10)
wine = read.csv("[10] Wine Quality.csv")
epsilon_ber <- 2*rbinom(100,1,p=0.5)-1     # converting the values 0, 1 to -1, 1
x_ber <- cumsum(epsilon_ber)
plot(x_ber, type='l')
epsilon_norm <- rnorm(100, 0, sd=0.1)
x_norm <- cumsum(epsilon_norm)
plot(x_norm, type='l')
N<-10000
epsilon1_ber <-  2 * rbinom(N,1,p=0.5) - 1
epsilon2_ber <-  2 * rbinom(N,1,p=0.5) - 1
x_ber <-  cumsum(epsilon1_ber)
y_ber <-  cumsum(epsilon2_ber)
plot(x_ber, y_ber, type='l')
N<-10000
epsilon1_norm <-  rnorm(N,0,0.1)
epsilon2_norm <-  rnorm(N,0,0.1)
x_norm <-  cumsum(epsilon1_norm)
y_norm <-  cumsum(epsilon2_norm)
plot(x_norm, y_norm, type='l')
library(quantmod)
sp500<-new.env()
?getSymbols
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2017-05-23')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)   # im confused it wont come out, why??????
getDividends('^GSPC', env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)
getSplits('^GSPC', env=sp500, src='yahoo',from=startDate,to=endDate,auto.assign=T)
head(sp500$GSPC)
tail(sp500$GSPC)
class(sp500$GSPC)
sp_prc<-sp500$GSPC[,6]   # becomes a time series of stock price
class(sp_prc)    # xts, zoo      'zoo' stands for Z's ordered observations
plot(sp_prc)
library(HSAUR)
install.packages('HSAUR')
library(HSAUR)
head(heptatholon)
head(heptathlon)
plot(heptathlon)
plot(heptathlon)
plot(heptathlon)
names(heptathlon)
colnames(heptathlon)
head(heptathlon2)
colnames(heptathlon)
heptathlon2 = heptathlon
heptathlon2$hurdles = max(heptathlon$hurdles) - heptathlon$hurdles
heptathlon2$run200m = max(heptathlon$run200m) - heptathlon$run200m
heptathlon2$run800m = max(heptathlon$run800m) - heptathlon$run800m
head(heptathlon2)
plot(heptathlon2)
nrow(heptathlon2)
fa.parallel(heptathlon2, #The data in question. (remember its covariance matrix)
n.obs = 25, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform. # why 100?
library(psych)
fa.parallel(heptathlon2, #The data in question. (remember its covariance matrix)
n.obs = 25, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform. # why 100?
abline(h = 1) #Adding a horizontal line at 1.
fa.parallel(heptathlon2[,-8], #The data in question. (remember its covariance matrix)
n.obs = 25, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform. # why 100?
abline(h = 1) #Adding a horizontal line at 1.
pc_heptathlon = principal(heptathlon2, #The data in question. (a covariance matrix...always?)
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_heptathlon
pc_heptathlon = principal(heptathlon2[,-8], #The data in question. (a covariance matrix...always?)
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_heptathlon
plot(pc_heptathlon)
factor.plot(pc_heptathlon,
labels = colnames(heptathlon[,-8])) #Add variable names to the plot.
plot(pc_heptathlon$scores, type = "n")
factor.plot(pc_heptathlon,
labels = colnames(heptathlon)) #Add variable names to the plot.
plot(pc_heptathlon$scores, type = "n")
text(pc_heptathlon$scores, rownames(heptathlon.new), cex = .75)
text(pc_heptathlon$scores, rownames(heptathlon2), cex = .75)
text(pc_heptathlon$scores, rownames(heptathlon2), cex = .5)
plot(pc_heptathlon$scores, type = "n")
text(pc_heptathlon$scores, rownames(heptathlon2), cex = .5)
View(heptathlon2)
factor.plot(pc_heptathlon,
labels = colnames(heptathlon[,-8])) #Add variable names to the plot.
plot(pc_heptathlon$scores, type = "n")
text(pc_heptathlon$scores, rownames(heptathlon2), cex = .5)
View(heptathlon2)
df = fread('df_important.csv')
View(df)
setwd("C:/Users/skick/Desktop/Kaggle")
df = fread('df_important.csv')
View(df)
df = fread('df_important.csv')
library(data.table)
df = fread('df_important.csv')
View(df)
